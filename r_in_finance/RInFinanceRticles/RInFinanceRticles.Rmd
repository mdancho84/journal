---
title: New Tools for Performing Financial Analysis Using the "Tidy" Split-Apply-Combine
  Framework
author:
  - name: Matt Dancho
    affiliation: Business Science
    email:  mdancho@business-science.io
  - name: Davis Vaughan
    affiliation: Business Science
    email:  dvaughan@business-science.io
abstract: >
  Financial analysis and data science in the R programming language have followed two separate yet innovative paths resulting in two different but important systems, "xts" and "tidyverse". Because of the separation in development, the two systems are difficult to use together, which limits the full potential of financial analysis within R. The `tidyquant` package solves this problem by integrating best financial analysis packages in the "xts" system with the "tidy" ecosystem, which unlocks the benefits of the _"split-apply-combine"_ framework and enables scaling financial analyses. 

    Two usage cases are discussed to illustrate the potential. The first example uses the scaling capabilities to provide an answer to how the "market" values risk versus reward. The second example evaluates the performance of multiple portfolios using weighted blends. These examples just scratch the surface of the full potential as technology develops, and the future possibilities are briefly addressed. 
preamble: >
output: rticles::rjournal_article_tq
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```

```{r, echo = FALSE}
# Load libraries
library(tidyquant)
library(knitr)

# Load Data Sets
sp500 <- readRDS("data/sp500.rds")
sp500_stock_prices <- readRDS("data/stock_prices.rds")
sp500_returns <- readRDS("data/stock_returns.rds")
sp500_stats <- readRDS("data/stats.rds")

FANG <- readRDS("data/fang.rds")
```


# Status of Financial Analysis Tools in R

The R programming language has seen immense growth in both popularity and tools over the past several years primarily driven by the open source nature of the R language and innovation in the field of data science. The sub-segment of financial analysis in R is no different. [TimelyPortfolio](https://timelyportfolio.github.io/rCharts_time_series/history.html) maintains a timeline of the major advances in R time series plotting, which highlights the inception of several of the most influential R financial and time series packages. Several of the influential packages are worth describing in more detail as these create much of the foundation of R in Finance currently.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=14cm]{img/timeline}
\end{figure}

## quantmod / TTR

The _"Quantitative Financial Modelling & Trading Framework for R"_ or `quantmod` package and the _"Technical Trading Rules"_ or `TTR` package include mechanisms to retrieve, compute, and visualize financial data using the most popular technical trading rules. 

## xts / zoo

The _"Extensible Time Series"_ or `xts` package along with the `zoo` package includes mechanisms for the handling of time series data. Most importantly, the `xts` package implemented a cross-package method for handling the various time-series data structures, in the process solving a major shortcoming by managing _all major R time series objects_ \footnote{At least the following time series data objects exist in R to solve general and specific needs: `fts`, `its`, `irts`, `timeSeries`, `ti`, `ts`, `mts`, `zoo`, and `xts`. The sheer volume of options results in a complex decision process to determine which to use. The `xts` package manages this complexity by providing one object class, `xts`, that consolidates the functionality of these options into one data type.} under one object class, `xts`.  

## PerformanceAnalytics

The `PerformanceAnalytics` package includes a large collection of econometric functions for financial performance analysis, many of which are described in _"Practical Portfolio Performance Measurement and Attribution"_ by Carl Bacon \citep{Bacon2004} . These functions enable analysis of individual asset and portfolio (the weighted aggregation of multiple assets) returns using popular statistical methods for measuring performance.

# New Tools: tidyverse

In parallel with the progression within the R in Finance community, developers at _RStudio_ have been developing useful tools for data science in R, namely the "tidyverse". The "tidyverse" or "tidy" ecosystem is a collection of packages that fundamentally and philosophically work together utilizing "tidy" data, which is defined in _"Tidy Data"_ \citep{tidy-data}. Further, the "tidyverse" packages and the data analysis workflow are documented in the online text, _"R for Data Science"_ \citep{R4DS2017}, which is the de facto manual for data scientists beginning with the R programming language. The "tidyverse" includes several packages worth describing in more detail.

## dplyr / tidyr

The `dplyr` and `tidyr` packages provide tools to clean and manipulate data using the _"split-apply-combine"_ framework popularized by Hadley Wickham in _"The Split-Apply-Combine Strategy for Data Analysis"_ \citep{plyr}. The major advances were threefold. First, the packages simplified and made consistent many of the most common data manipulation and summarization tasks in R. Second, the packages enable the _"split-apply-combine"_ framework on grouped data sets. Third, the packages incorporated the use of the pipe (`%>%`) from the `magrittr` package enabling the functional verbs to follow an easy, efficient, and human-readable workflow. 

## purrr

The `purrr` package provides tools for applying functions to data frames. Similar to the traditional, `apply` function in base R, the `map` function enables "mapping"" functions row-wise within a data frame. The major advance is the ability to scale analysis. An analysis can be performed with "nested" data frames allowing users to apply functions across many, independent data sets, which is a common task in data science. Further, the resulting data frame is "tidy", meaning each analysis result is kept alongside the data that generated it.

## tibble

The `tibble` package extends the traditional `data.frame` object by providing useful tools for creating and coercing objects to "tibbles" or "tidy" data frames.

## ggplot2

The `ggplot2` package provides mechanisms for creating complex visualizations using a layered approach called the "grammar of graphics", which is discussed in _"A Layered Grammar of Graphics"_ \citep{layered-grammar}. The `ggplot2` package is the primary package for creating static graphics in the "tidy" ecosystem.

## lubridate

The `lubridate` package includes functions to manage date and date-time objects in R, which is discussed in _"Dates and Times Made Easy with lubridate"_ \citep{lubridate}. The combination of using `lubridate` with `dplyr` enables coercing character class to date and date-time, filtering and subsetting on dates, and many more complex tasks that are essential to financial analysis. Further, combining `lubridate` with `ggplot2` enables graphical visualization using dates and date-times.   


# Divergent Philosophies, Each with Advantages

All of the major financial packages work within the "xts" system, which is specifically designed for time series analysis. The system works very well. The extensible time series, "xts", data structure is much like a numeric matrix, the only major difference being row names consisting of the date or date-time information. The advantage of the "xts" system is the ability to manage numeric data with date or date-time references. The objects can be subset or transformed to different periodicity very easily. The disadvantage is the application is very strict to numeric data, but because of its focus it manages the numeric, time-based data extremely well.

Much of the recent innovation in data analysis has occurred within the "tidy" ecosystem. With the entrance of the "tidyverse", scaling analysis using the _"split-apply-combine"_ framework has become easy, efficient, and core functionality. Further, advances in date and date-time functionality has enabled management of time series data within the "tibble" ("tidy" data frame) data structure. As the data science field grows, more innovative functionality will continue to be developed within the "tidy" ecosystem, much of which will be (and is already) useful to the field of financial analysis.

The two systems, "xts" and the "tidyverse", are very different on a fundamental and philosophical level. Both have advantages that are needed within the realm of financial analysis, but unfortunately the two systems do not work well together. The "xts" system is strictly numeric-based, while the "tidy" system is strictly data frame-based. Passing data between systems is difficult if not painful, and without communication between each the full potential of financial analysis within R is limited.

To solve this problem, the `tidyquant` package was developed as a way to integrate many of the "xts" based financial analysis packages within the "tidyverse". The central idea is to integrate the two systems enabling the user to gain the benefits of both systems. The `tidyquant` package works with "tidy" data frame input and output, and as a result fits seamlessly within the "tidy" ecosystem allowing for analysis to follow the data science workflow discussed in detail in _"R for Data Science"_ \citep{R4DS2017}. Internally, `tidyquant` leverages the "xts" system as the engine to perform financial computations. This enables almost all of the functionality of `quantmod`, `TTR`, `PerformanceAnalytics`, `xts` and `zoo` to be used integrally (without switching back and forth) within the "tidy" ecosystem. The primary benefit of this integration is the ability to implement the _"split-apply-combine"_ framework to scale complex analysis. 

In the next section, the _"split-apply-combine"_ framework is discussed conceptually using non-financial data and then a demonstration of the `tidyquant` package is presented to illustrate some of the key benefits of the integration of "xts" based financial packages into the "tidy" ecosystem within the realm of financial analysis.

# Split-Apply-Combine, The Concept

The _"split-apply-combine"_ framework is described in _"The Split-Apply-Combine Strategy for Data Analysis"_ \citep{plyr}. To summarize, the core concept is to split a data set into groups, apply functions to independent groups, and then recombine the results. The value in this approach is the framework enables scaling analysis from one group to many groups and comparing each group to each other. 

A simple example using the `mtcars` data set illustrates the _"split-apply-combine"_ framework. The `mtcars` data set includes the various attributes for 32 vehicles along with the fuel efficiency (MPG) of each vehicle. Start with a question: _How does engine size affect fuel consumption?_

Load the `tidyverse` and the `mtcars` data set in R.

```{r}
library(tidyverse)
data("mtcars")
```

Next, view the data set. The `as_tibble` function is used to convert to the "tidy" data frame structure. The data set consists of 12 columns (features) and 32 rows (observations) of related to various automobiles. Fortunately, the frame of the problem statement narrows the analysis to two variables: "mpg", continuous numeric data describing the fuel efficiency of each vehicle, and "cyl", discrete numeric data indicating number of engine cylinders for each vehicle. The "cyl" data can be grouped on to keep like observations together.

```{r, eval = FALSE}
mtcars <- mtcars %>%
    rownames_to_column(var = "model") %>%
    as_tibble()
head(mtcars)
```

```{r, eval = TRUE, results="asis", echo=FALSE}
# Run. Only output shown
mtcars <- mtcars %>%
    rownames_to_column(var = "model") %>%
    as_tibble()
kable(head(mtcars), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

Many solutions exist to compare fuel consumption and engine size. For simplicity, the mean and standard deviation are used to characterize the relationship between number of cylinders, "cyl", and miles per gallon, "mpg". Implementing _"split-apply-combine"_ is as easy as grouping by a categorical variable and summarizing by the target measure. In this case, the categorical variable is the "cyl" variable.

```{r, eval=FALSE}
mtcars %>%
    mutate(cyl = factor(cyl)) %>%
    group_by(cyl) %>%
    summarize(mpg.mean = mean(mpg),
              mpg.sd = sd(mpg))
```

```{r, eval=TRUE, results="asis", echo=FALSE}
mtcars %>%
    mutate(cyl = factor(cyl)) %>%
    group_by(cyl) %>%
    summarize(mpg.mean = mean(mpg),
              mpg.sd = sd(mpg)) %>%
    kable(format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

From the results, the vehicles, when grouped by "cyl", appear to have an inverse relationship between number of cylinders and miles per gallon. We can also visualize this relationship using `ggplot2`.

```{r, fig.width=5, fig.height=3.5, fig.align="center"}
mtcars %>%
    mutate(cyl = factor(cyl)) %>%
    ggplot(aes(x = cyl, y = mpg, fill = cyl)) +
    geom_boxplot() +
    labs(title = "Summarizing Relationships using Split-Apply-Combine",
         subtitle = "Inverse Relationship between Engine Size and Fuel Efficiency",
         x = "Engine Size (No. of Cylinders)",
         y = "Fuel Efficiency (MPG)")
```

The _"split-apply-combine"_ framework is very useful to solve a wide range of complex problems. In the next section, examples are presented to illustrate the power of unlocking the framework along with the data science workflow tools within financial analysis applications.


# Split-Apply-Combine, Applications in Finance

The R financial analysis packages are difficult or impossible to use within the "tidy" ecosystem. A new tool is needed, `tidyquant`. The `tidyquant` package has one major advance: it integrates the "xts" based financial packages with the "tidyverse" to enable the data science workflow and "tidy" ecosystem functionality to be applied to financial analysis. The innovation is relatively minor on a conceptual level, but the net effect is significant in that the tools within the "tidy" ecosystem are now unlocked for financial analysis. The following examples illustrate the new capability.

## Example 1: Evaluating Risk vs Reward

Financial analysis is almost always a trade off between risk and reward. Reward is measured by growth of an investment. Risk is typically associated with volatility. Often when beginning an analysis, one wishes to understand the components of a "market" basket of stocks on the basis of this risk-reward trade off in order to screen investments. Start with a question: _How does the "market" value risk versus reward?_

This question is answerable by comparing the risk and reward characteristics for a basket of stocks viewed as a reasonable proxy for the "market". The observations within the market are stocks. Stocks also have observations or historical prices, which can be obtained over time and can be used to evaluate various statistical qualities that relate to risk versus reward. The reward is the return performance (i.e. returns), which is the positive or negative percentage change between observations. When accumulated across a frequency such as daily, the large set of returns has statistical properties that can be measured. To simplify the question, the average and standard deviation of the returns can be used as a general proxy of risk versus reward. When pooled together, the stocks can thus be compared to expose general performance trends withing the "market". \footnote{Using the mean and standard deviation of stock returns has roots in Brownian motion and is known as the Stochastic Process, which is often used in Monte Carlo simulation to predict the range of future returns within a confidence interval based on the properties of the past returns. }

To start, load `tidyquant`, which loads all of the packages needed to evaluate risk versus reward.

```{r, eval = F}
# Loads tidyquant, tidyverse, lubridate, quantmod, TTR, xts, zoo, PerformanceAnalytics
library(tidyquant)
```

Next, collect some financial data. The question implies that a large sample of stock data is needed to evaluate how performance and risk is valued within the "market". The SP500 index is a good place to start. `tidyquant` includes a function, `tq_index`, which returns the stock symbols and company names for every stock within an index.

```{r, eval = FALSE}
sp500 <- tq_index("SP500")
sp500
```

```{r, eval=TRUE, results="asis", echo=FALSE}
kable(head(sp500), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

Next, we need the historical stock prices for each stock within the SP500 index. The stock prices are easy to retrieve at scale using the function, `tq_get`, with the "get" option, `get = "stock.prices"`. `tq_get` is a wrapper for `quantmod::getSymbols`, which enables passing the underlying function parameters `from` and `to`. The input to `tq_get` is `data`, which can be a single stock symbol, a vector of stock symbols, or a data frame with stock symbols in the first column. The latter is passed to `tq_get` using the pipe (`%>%`). The function may take a few minutes to run because it is downloading the past ten years of daily open, high, low, close, volume, and adjusted stock prices for the entire SP500 index into one "tidy" data frame. Reviewing the results indicates that the prices were retrieved in entirety. The resulting "tibble" has `r nrow(sp500_stock_prices) %>% scales::comma()` rows and `r sp500_stock_prices$symbol %>% unique() %>% length()` unique symbols. 



```{r, eval = F}
stock_prices <- sp500 %>%
    tq_get(get  = "stock.prices", 
           from = "2007-01-01", 
           to   = "2017-01-01")
sp500_stock_prices
```

```{r, eval=TRUE, results="asis", echo=FALSE}
kable(head(sp500_stock_prices), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

Next, the _"split-apply-combine"_ framework is used to group the prices by stock symbol and to calculate the logarithmic daily returns. The transform is applied using `tq_transform`, which is used in situations where periodicity changes (or can change). The `quantmod` OHLC (open, high, low, close) notation is used to collect the adjusted prices (`ohlc_fun = Ad`) and send these prices to the `periodReturn` function (`transform_fun = periodReturn`). The additional arguments `period = "daily"` and `type = "log"` are passed to the transformation function, `periodReturn`. The daily log returns (DLR) for each of the `r sp500_returns$symbol %>% unique() %>% length()` groups of stock symbols is generated below.

```{r, eval = F}
sp500_returns <- sp500_stock_prices %>%
    group_by(symbol) %>%
    tq_transform(ohlc_fun = Ad, transform_fun = periodReturn, 
                 period = "daily", type = "log", col_rename = "dlr")
sp500_returns
```

```{r, eval=TRUE, results="asis", echo=FALSE}
kable(head(sp500_returns), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

Next, the mean and standard deviation of the daily log returns are used to evaluate and compare the stocks. The easiest way is to use `tq_performance`, which enables applying the `PerformanceAnalytics` performance functions to "tidy" data frames of asset or portfolio returns. The `table.Stats` function returns the arithmetic mean and standard deviation along with a number of other useful statistics that characterize the returns.

```{r, eval = F}
sp500_stats <- sp500_returns %>%
    tq_performance(Ra = dlr, performance_fun = table.Stats, ci = 0.95, digits = 6) 
sp500_stats
```

```{r, eval=TRUE, results="asis", echo=FALSE}
kable(head(sp500_stats[,1:5]), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

Finally, we have the data needed to visualize risk versus reward. A plot of the mean daily log returns (MDLR) versus the standard deviation of daily log returns (SDDLR) shows the relationship. Note that observations (stocks) with fewer than five years or trading days (5 years x 252 trade days per year) are removed from the visualization to yield a long-run perspective. 

```{r, fig.width=5, fig.height=3.5, fig.align="center"}
sp500_stats %>%
    filter(Observations >= 252 * 5) %>%
    ggplot(aes(x = Stdev, y = ArithmeticMean)) +
    geom_point(alpha = 0.5, col = "steelblue") +
    geom_smooth(method = "lm") +
    labs(title = "Evaluating Risk Vs Reward for Stocks in the SP500",
         subtitle = "Split-Apply-Combine Enables Scaling Financial Analysis",
         caption = "An inverse trend between volatility (SDDLR) and returns (MDLR)",
         x = "Standard Deviation of Daily Log Returns (SDDLR)",
         y = "Arithmetic Mean of Daily Log Returns (MDLR)") +
    theme_tq()
```

By comparing an entire basket of stocks, reasonable trading strategies can be developed. For example, creating a portfolio that minimizes volatility may result in higher performance over the long run. This is easily seen in the trend line indicating an inverse trend between volatility (SDDLR) and returns (MDLR). Further, once trading strategies are developed, screening is easily implemented by filtering the data set.

```{r, eval=FALSE}
sp500_stats %>%
    filter(Observations >= 252 * 5,
           Stdev <= 0.02,
           ArithmeticMean >= 0.0005) %>%
    arrange(desc(ArithmeticMean))
```

```{r, echo=FALSE, results="asis"}
x <- sp500_stats %>%
    filter(Observations >= 252 * 5,
           Stdev <= 0.02,
           ArithmeticMean >= 0.0005) %>%
    arrange(desc(ArithmeticMean)) %>%
    .[,1:5]

kable(head(x), format = "latex", align = 'c', booktabs = TRUE, digits = 5)
```

\hspace{20 mm}

## Example 2: Evaluating Performance of Multiple Portfolio Blends

Portfolio aggregation is a useful technique to reduce risk while maintaining acceptable returns. In this example, the goal is to evaluate a few blended portfolios of "FANG" stocks ("FB", "AMZN", "NFLX", and "GOOG"). \footnote{The acronymn, "FANG", was popularized by Jim Cramer of the popular CNBC show, Mad Money. http://www.investopedia.com/terms/f/fang-stocks-fb-amzn.asp.} However, more important is the conceptual idea that the number of weighted portfolio blends and the number of underlying assets can easily be increased to scale the analysis. As the example progresses, consider how this could be applied to an entire index of assets and hundreds of weighted blends.  

Throughout the past half decade, the FANG stocks have experienced a unique combination of high returns and high volatility, making the FANG stocks a good example to use in a blended portfolio that reduces downside risk but yields high returns. Start with a question: _What portfolio blends reduce downside risk while maximizing return?_

Several portfolio blends will be evaluated to see the historical effect on returns over a since 2013: \footnote{2013 was the first full year of trading data for FB. Portfolio aggregation prior to 2013 cannot be analyzed with FB.}

* Portfolio 1: 50% FB, 25% AMZN, 25% NFLX, 0% GOOG
* Portfolio 2: 0%  FB, 50% AMZN, 25% NFLX, 25% GOOG
* Portfolio 3: 25% FB, 0%  AMZN, 50% NFLX, 25% GOOG
* Portfolio 4: 25% FB, 25% AMZN, 0%  NFLX, 50% GOOG

First, collect the data for the "FANG" stocks.

```{r, eval = F}
FANG <- c("FB", "AMZN", "GOOG", "NFLX") %>%
    tq_get(get = "stock.prices")
FANG
```

```{r, eval=TRUE, results="asis", echo=FALSE}
FANG <- c("FB", "AMZN", "GOOG", "NFLX") %>%
    tq_get(get = "stock.prices")
kable(head(FANG), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

Next, transform to monthly returns using the _"split-apply-combine"_ framework. Use `group_by` to group on the "symbol" column, and `tq_transform` to transform the adjusted prices to monthly arithmetic returns. Note that "FB" was actively traded for a full year beginning in 2013, so it makes sense to compare the investments since then.

```{r, eval = FALSE}
FANG_returns <- FANG %>%
    filter(date >= ymd("2013-01-01"),
           date <  ymd("2017-01-01")) %>%
    group_by(symbol) %>%
    tq_transform(ohlc_fun = Ad,
                 transform_fun = periodReturn,
                 period = "monthly",
                 col_rename = "returns")
FANG_returns
```

```{r, eval=TRUE, results="asis", echo=FALSE}
FANG_returns <- FANG %>%
    filter(date >= ymd("2013-01-01"),
           date <  ymd("2017-01-01")) %>%
    group_by(symbol) %>%
    tq_transform(ohlc_fun = Ad,
                 transform_fun = periodReturn,
                 period = "monthly",
                 col_rename = "returns")
kable(head(FANG_returns), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

### Individual Asset Performance

Before the portfolios are generated and evaluated, it makes sense to visualize and assess the performance of the individual asset returns. The first visualization uses a wealth index, which takes an initial investment as an input and returns a visualization showing how the investment would have grown over the specified time period. From the visualization, NFLX was the best performer, but it also experienced some significant drops along the way. 

```{r, fig.width=5, fig.height=3.5, fig.align="center"}
init_investment <- 10000
FANG_wealth <- FANG_returns %>%
    mutate(wealth.index = init_investment * cumprod(1 + returns))

FANG_wealth %>%
    ggplot(aes(x = date, y = wealth.index, color = symbol)) +
    geom_line(size = 2) +
    geom_smooth(method = "loess") +
    labs(title = "Individual Stocks: Comparing the Growth of 10K",
         x = "", y = "Investment Value") +
    theme_tq() +
    scale_color_tq() +
    scale_y_continuous(labels = scales::dollar)
```

Because of the volatility, the risk should be evaluated as well. One popular method to measure risk is using value at risk (VaR). VaR measures the worst expected loss over a given time interval under normal market conditions at a given confidence level \citep{Bacon2004}. Implementing VaR can be done using `tq_performance` with the `PerformanceAnalytics` function, `VaR`, to estimate risk measures across the FANG stocks. While NFLX and AMZN returns are stellar, the VaR risk estimate is also double that of FB and GOOG.

```{r, eval=FALSE}
VaR_FANG <- FANG_returns %>%
    tq_performance(Ra = returns, Rb = NULL, performance_fun = VaR, p = 0.95) %>%
    rename(VaR.monthly = VaR) 
VaR_FANG
```

```{r, eval=TRUE, results="asis", echo=FALSE}
VaR_FANG <- FANG_returns %>%
    tq_performance(Ra = returns, Rb = NULL, performance_fun = VaR, p = 0.95) %>%
    rename(VaR.monthly = VaR) 
kable(VaR_FANG, format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

From the results, a portfolio consisting entirely of one of the assets could yield great results, but it's not for the risk averse. A better method might be to use a weighted aggregation within a portfolio.

### Portfolio Performance

Weighted portfolio aggregation involves three steps:

1. Make a portfolio by repeating the stock returns table _n_ times 
2. Create a weights table to map weights by asset and portfolio
3. Aggregate the portfolios using `tq_portfolio`, a wrapper for `PerformanceAnalytics::Return.portfolio`

#### Step 1: Make a portfolio by repeating the stock returns

Use `tq_repeat_df` to repeat `n = 4` times. This function grows the data frame row-wise, adding an index column named "portfolio" and grouping by "portfolio". We now have four portfolio groups that will be evaluated. 

``` {r, eval=FALSE}
FANG_returns_mult <- FANG_returns %>%
    tq_repeat_df(n = 4)
FANG_returns_mult
```

```{r, eval=TRUE, results="asis", echo=FALSE}
FANG_returns_mult <- FANG_returns %>%
    tq_repeat_df(n = 4)
kable(head(FANG_returns_mult), format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

#### Step 2: Create a Weights Table

Construct a weights table using the portfolio blending parameters, which will be used to map to the portfolios in the next step. The format of the weights table is critical. The table must have three columns, "portfolio", "stocks", and "weights". Make sure to group by the portfolio column.

* Portfolio 1: 50% FB, 25% AMZN, 25% NFLX, 0% GOOG
* Portfolio 2: 0% FB, 50% AMZN, 25% NFLX, 25% GOOG
* Portfolio 3: 25% FB, 0% AMZN, 50% NFLX, 25% GOOG
* Portfolio 4: 25% FB, 25% AMZN, 0% NFLX, 50% GOOG

``` {r, eval=FALSE}
weights <- c(0.50, 0.25, 0.25, 0.00,
             0.00, 0.50, 0.25, 0.25,
             0.25, 0.00, 0.50, 0.25, 
             0.25, 0.25, 0.00, 0.50)
weights_table <- tibble(stocks = c("FB", "AMZN", "NFLX", "GOOG")) %>%
    tq_repeat_df(n = 4) %>%
    bind_cols(tibble(weights)) %>%
    group_by(portfolio)
weights_table
```

```{r, eval=TRUE, results="asis", echo=FALSE}
weights <- c(0.50, 0.25, 0.25, 0.00,
             0.00, 0.50, 0.25, 0.25,
             0.25, 0.00, 0.50, 0.25, 
             0.25, 0.25, 0.00, 0.50)
weights_table <- tibble(stocks = c("FB", "AMZN", "NFLX", "GOOG")) %>%
    tq_repeat_df(n = 4) %>%
    bind_cols(tibble(weights)) %>%
    group_by(portfolio)
kable(weights_table, format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

#### Step 3: Aggregate the Portfolios with tq_portfolio

Aggregate the portfolios using `tq_portfolio`, a wrapper for `PerformanceAnalytics::Return.portfolio`. The `Return.portfolio` has additional arguments to create a wealth index, which is the compounded returns. Setting `wealth.index = TRUE` and multiplying the result by the initial investment value of $10,000 returns a wealth index. The performance of the various blended portfolios is visualized using `ggplot2` with the aesthetic argument, `color = factor(portfolio)`.

``` {r, fig.width=5, fig.height=3.5, fig.align="center"}
# Aggregate portfolio with tq_portfolio. Pass wealth.index = TRUE
init_investment <- 10000
FANG_portfolio_wealth <- FANG_returns_mult %>%
    tq_portfolio(assets_col = symbol, returns_col = returns,
                 weights = weights_table, wealth.index = TRUE,
                 col_rename = "wealth.index") %>%
    mutate(wealth.index = wealth.index * init_investment)

FANG_portfolio_wealth  %>%
    ggplot(aes(x = date, y = wealth.index, color = factor(portfolio))) +
    geom_line(size = 2) +
    geom_smooth(method = "loess") +
    labs(title = "Portfolios: Comparing the Growth of 10K",
         subtitle = "Quickly visualize blended portfolio performance",
         x = "", y = "Investment Value",
         color = "Portfolio Number: ") +
    theme_tq() +
    scale_color_tq() +
    scale_y_continuous(labels = scales::dollar)
```

Finally, the risk is assessed in the same manner as the individual assets. The portfolio aggregation is performed without the `wealth.index` option, which aggregates uncompounded returns. The returns are "piped" to the `tq_performance` function, which returns the VaR. 

```{r, eval=FALSE}
VaR_portfolio <- FANG_returns_mult %>%
    tq_portfolio(assets_col = symbol, returns_col = returns,
                 weights = weights_table, col_rename = "returns") %>%
    tq_performance(Ra = returns, Rb = NULL, performance_fun = VaR) %>%
    rename(VaR.monthly = VaR)
VaR_portfolio
```

```{r, eval=TRUE, results="asis", echo=FALSE}
VaR_portfolio <- FANG_returns_mult %>%
    tq_portfolio(assets_col = symbol, returns_col = returns,
                 weights = weights_table, col_rename = "returns") %>%
    tq_performance(Ra = returns, Rb = NULL, performance_fun = VaR) %>%
    rename(VaR.monthly = VaR)
kable(VaR_portfolio, format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

The two tables below summarize the results of the individual and portfolio analyses, respectively. The Portfolio 1 blend is an attractive combination of growth with lower risk: over 5.1X return with over a 1% decrease in the VaR of both NFLX and AMZN.  

\hspace{20 mm}

```{r, echo = F, eval=FALSE}
FANG_wealth %>% 
    summarize(value.end = last(wealth.index)) %>%
    left_join(VaR_FANG, by = "symbol")
```

```{r, eval=TRUE, results="asis", echo=FALSE}
FANG_wealth %>% 
    summarize(value.end = last(wealth.index)) %>%
    left_join(VaR_FANG, by = "symbol") %>%
    kable(format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

```{r, echo = F, eval=FALSE}
FANG_portfolio_wealth %>%
    summarize(value.end = last(wealth.index)) %>%
    left_join(VaR_portfolio, by = "portfolio")
```

```{r, eval=TRUE, results="asis", echo=FALSE}
FANG_portfolio_wealth %>%
    summarize(value.end = last(wealth.index)) %>%
    left_join(VaR_portfolio, by = "portfolio") %>%
    kable(format = "latex", align = 'c', booktabs = TRUE)
```

\hspace{20 mm}

# Future Possibilities

The _"split-apply-combine"_ framework presents an opportunity for progress in the realm of financial analysis due to the scale at which analysis can now be performed. Portfolio analysis is especially interesting because as the number of components and the number of portfolio blend variations increase, the number of portfolio combinations become infinite. Yet, with the scaling capabilities within the "tidy" ecosystem, the vast array of possibilities becomes limited only by the data storage and processing capabilities of the computers on which the computations are performed. With advances in cloud technology, parallel processing technology, and optimization algorithms, the tools are available to scale analyses to terabytes worth of financial data in real time. The work is not complete, but, as innovation continues in the realm of data science, the realm of financial analysis will benefit in scale and speed. 

\bibliography{RJreferences}